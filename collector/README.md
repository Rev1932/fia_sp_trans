# üöå Coletor de Dados - API Olho Vivo (SPTrans)

Este projeto √© um microservi√ßo de coleta de dados de alta disponibilidade, projetado para consumir a API **Olho Vivo da SPTrans** em tempo real, transformar os dados e envi√°-los como um *Producer* para t√≥picos do **Apache Kafka**.

O servi√ßo foi arquitetado para ser um *daemon* (servi√ßo de longa dura√ß√£o), gerenciado por um *launcher* em shell que garante sua execu√ß√£o cont√≠nua. A l√≥gica de transforma√ß√£o de JSONs aninhados da API √© gerenciada de forma gen√©rica usando a biblioteca **Pandas**, especificamente a fun√ß√£o `json_normalize`.

## üèõÔ∏è Arquitetura

O projeto √© composto por 5 componentes principais que trabalham juntos:

1.  **`main.py` (O Servi√ßo)**: O cora√ß√£o do aplicativo. Este script √© um servi√ßo de longa dura√ß√£o que roda em um loop infinito. Ele √© respons√°vel por:
    * Manter a autentica√ß√£o com a API.
    * Ler o arquivo `endpoints.txt` a cada ciclo.
    * Chamar a API para cada *alias* configurado.
    * Usar o Pandas para transformar as respostas JSON em DataFrames "achatados".
    * Converter os DataFrames de volta para `list[dict]`.
    * Enviar as mensagens para o Kafka usando a classe `KafkaProducerUtil`.

2.  **`kafka_utils.py` (O Conector Kafka)**: Um m√≥dulo utilit√°rio que define a classe `KafkaProducerUtil`. Esta classe gerencia uma **conex√£o persistente** com o broker Kafka, lidando com o envio de mensagens, *batching* e fechamento limpo da conex√£o.

3.  **`executar_coleta.sh` (O Launcher Resiliente)**: Um script de *shell* que atua como "guardi√£o" do `main.py`. Ele usa um loop `until` para iniciar o servi√ßo Python e, o mais importante, **reinici√°-lo automaticamente** se o script falhar por qualquer motivo (erro de rede, falha na API, etc.), garantindo que o coletor esteja sempre rodando.

4.  **`endpoints.txt` (O Arquivo de Configura√ß√£o)**: O "c√©rebro" da coleta. Em vez de usar `argparse`, este arquivo de texto simples define quais *aliases* de coleta devem ser executados a cada ciclo. O `main.py` l√™ este arquivo para saber o que fazer.

5.  **`.env` (As Credenciais)**: Um arquivo de ambiente (padr√£o *dotenv*) para armazenar credenciais sens√≠veis (token da API, endere√ßo do Kafka) de forma segura, fora do c√≥digo-fonte.

## üìã Pr√©-requisitos

* Python 3.9+
* Uma inst√¢ncia do Apache Kafka acess√≠vel.
* Credenciais v√°lidas para a API Olho Vivo (SPTrans).

## üöÄ Instala√ß√£o e Configura√ß√£o

1.  Clone este reposit√≥rio:
    ```bash
    git clone <url-do-seu-repositorio>
    cd <nome-do-repositorio>
    ```

2.  Crie um ambiente virtual (recomendado):
    ```bash
    python3 -m venv venv
    source venv/bin/activate
    ```

3.  Instale as depend√™ncias. Crie um arquivo `requirements.txt` com o seguinte conte√∫do:
    **`requirements.txt`**
    ```text
    requests
    kafka-python
    pandas
    python-dotenv
    ```
    Em seguida, instale-o:
    ```bash
    pip install -r requirements.txt
    ```

4.  Crie e configure seu arquivo `.env`:
    **`.env`**
    ```ini
    # Token da API Olho Vivo
    SPTRANS_API_TOKEN="SEU_TOKEN_AQUI"

    # Endere√ßo dos seus brokers Kafka
    KAFKA_BOOTSTRAP_SERVERS="localhost:9092"

    # (Opcional) Tempo em segundos entre cada chamada de API (para evitar Rate Limit)
    API_RATE_LIMIT_SECONDS=5

    # (Opcional) Tempo em segundos de espera entre ciclos de coleta completos
    CYCLE_SLEEP_SECONDS=300
    ```

5.  Configure seus *aliases* de coleta no `endpoints.txt`:
    **`endpoints.txt`**
    ```ini
    # Este arquivo define quais coletas ser√£o executadas.
    # O main.py ir√° ler cada linha e process√°-la.
    # Coment√°rios (com #) e linhas em branco s√£o ignorados.

    # Coletas de Posi√ß√£o
    posicao_frota
    posicao_linha_5111
    posicao_linha_31398 # (Letreiro 2766-10)

    # Coletas de Dados Cadastrais (rodar com menos frequ√™ncia)
    empresas_todas
    corredores_todos

    # Coletas de Previs√£o
    previsao_parada_4200052
    ```

## üëü Como Executar

O servi√ßo √© iniciado usando o *script* de *shell* `executar_coleta.sh`.

1.  Torne o script execut√°vel (apenas na primeira vez):
    ```bash
    chmod +x executar_coleta.sh
    ```

2.  Inicie o servi√ßo:
    ```bash
    ./executar_coleta.sh
    ```
    O script come√ßar√° a rodar e permanecer√° no seu terminal. Se ele falhar, o *script* o reiniciar√° automaticamente ap√≥s 10 segundos. Para parar, pressione `Ctrl+C`.

3.  (Opcional) Executando em Background (Modo de Produ√ß√£o)
    Para rodar o servi√ßo permanentemente em um servidor, use `nohup` e `&`:
    ```bash
    nohup ./executar_coleta.sh > coletor.log 2>&1 &
    ```
    * `nohup`: Garante que o script continue rodando mesmo se voc√™ fechar o terminal.
    * `> coletor.log 2>&1`: Redireciona toda a sa√≠da (logs e erros) para o arquivo `coletor.log`.
    * `&`: Coloca o processo em segundo plano.

---

## üîß Configura√ß√£o Avan√ßada (Adicionando Novos Endpoints)

A intelig√™ncia do coletor est√° na capacidade de adicionar novos *endpoints* sem reescrever a l√≥gica de transforma√ß√£o. Isso √© feito editando duas fun√ß√µes no `main.py`:

1.  **`rotear_chamada(alias)`**: Mapeia um *alias* do `endpoints.txt` para uma URL, um t√≥pico Kafka e uma configura√ß√£o de transforma√ß√£o (o `pd_config`).
2.  **`transformar_para_dataframe(json_data, pd_config)`**: Usa a configura√ß√£o do `pd_config` para achatar o JSON usando `pd.json_normalize`.

### O `pd_config`

O `pd_config` √© um dicion√°rio que passa argumentos diretamente para `pd.json_normalize`:

* `record_path`: O caminho (uma lista) at√© a **lista de dados** que voc√™ quer transformar em **linhas** no DataFrame.
* `meta`: Uma lista de chaves do JSON "pai" que voc√™ quer **copiar** para cada nova linha (ex: copiar o `codigo_linha` para cada ve√≠culo).
* `rename_map`: (Opcional) Um dicion√°rio para renomear as colunas da API (ex: `py` -> `latitude`).

### Exemplo: Adicionando o Endpoint `/Posicao/Garagem`

**1. JSON de Exemplo (API):**
```json
{
  "hr": "10:30",
  "e": [
    { "c": 1, "a": 11, "v": [ {"p": "1 1001", "py": -23.1}, ... ] },
    { "c": 2, "a": 8,  "v": [ {"p": "2 1501", "py": -23.5}, ... ] }
  ]
}